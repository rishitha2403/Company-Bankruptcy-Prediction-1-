{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARGfZJXx-NXa",
        "outputId": "35020e3f-a6b4-48b3-88d3-dfdfa1acd04d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Best threshold: 0.37 with F1-Score: 0.4262\n",
            "\n",
            "Final Evaluation:\n",
            "Accuracy:  0.9489\n",
            "Precision: 0.4643\n",
            "Recall:    0.3939\n",
            "F1 Score:  0.4262\n",
            "ROC-AUC:   0.8825\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      1303\n",
            "           1       0.46      0.39      0.43        66\n",
            "\n",
            "    accuracy                           0.95      1369\n",
            "   macro avg       0.72      0.69      0.70      1369\n",
            "weighted avg       0.95      0.95      0.95      1369\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1273   30]\n",
            " [  40   26]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n",
        "                             recall_score, f1_score, classification_report, roc_auc_score)\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "# 1. Load Test Data\n",
        "\n",
        "test_data_path = \"\"  # Add the test data set here\n",
        "test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "# Separate features and target\n",
        "X_test = test_df.drop(columns=['Bankrupt?'])\n",
        "y_test = test_df['Bankrupt?']\n",
        "\n",
        "\n",
        "# 2. Feature Selection\n",
        "\n",
        "selected_features = [' Cash flow rate',\n",
        " ' Tax rate (A)',\n",
        " ' Net Value Per Share (A)',\n",
        " ' Persistent EPS in the Last Four Seasons',\n",
        " ' Cash Flow Per Share',\n",
        " ' Operating Profit Per Share (Yuan ¥)',\n",
        " ' Total debt/Total net worth',\n",
        " ' Debt ratio %',\n",
        " ' Borrowing dependency',\n",
        " ' Contingent liabilities/Net worth',\n",
        " ' Operating profit per person',\n",
        " ' Working Capital to Total Assets',\n",
        " ' Quick Assets/Total Assets',\n",
        " ' Cash/Total Assets',\n",
        " ' Current Liability to Assets',\n",
        " ' Operating Funds to Liability',\n",
        " ' Working Capital/Equity',\n",
        " ' Long-term Liability to Current Assets',\n",
        " ' Retained Earnings to Total Assets',\n",
        " ' Total expense/Assets',\n",
        " ' Fixed Assets to Assets',\n",
        " ' Equity to Long-term Liability',\n",
        " ' Cash Flow to Total Assets',\n",
        " ' CFO to Assets',\n",
        " ' Current Liability to Current Assets',\n",
        " ' Liability-Assets Flag',\n",
        " ' Net Income to Total Assets',\n",
        " ' Gross Profit to Sales',\n",
        " \" Net Income to Stockholder's Equity\",\n",
        " ' Equity to Liability']\n",
        "\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "\n",
        "# 3. Scale the Test Data\n",
        "\n",
        "if os.path.exists('scaler.pkl'):\n",
        "    with open('scaler.pkl', 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    X_test_scaled = scaler.transform(X_test_selected)\n",
        "else:\n",
        "    raise FileNotFoundError(\"scaler.pkl not found. Ensure the scaler file is available.\")\n",
        "\n",
        "\n",
        "# 4. Load the Saved Models\n",
        "\n",
        "if os.path.exists('dnn_model.h5'):\n",
        "    dnn_model = load_model('dnn_model.h5')\n",
        "else:\n",
        "    raise FileNotFoundError(\"dnn_model.h5 not found. Ensure the model file is available.\")\n",
        "\n",
        "if os.path.exists('GaussianNB_model (1).pkl'):\n",
        "    with open('GaussianNB_model (1).pkl', 'rb') as f:\n",
        "        gnb_model = pickle.load(f)\n",
        "else:\n",
        "    raise FileNotFoundError(\"GaussianNB_model.pkl not found. Ensure the model file is available.\")\n",
        "\n",
        "\n",
        "# 5. Generate Predictions\n",
        "\n",
        "y_pred_gnb_prob = gnb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred_dnn_prob = dnn_model.predict(X_test_scaled).flatten()\n",
        "\n",
        "# Combine predictions using soft voting\n",
        "combined_prob = (y_pred_gnb_prob + y_pred_dnn_prob) / 2\n",
        "\n",
        "# Tune threshold for best F1-score\n",
        "thresholds = np.arange(0.30, 0.60, 0.01)\n",
        "best_f1 = 0\n",
        "best_thresh = 0.5\n",
        "for thresh in thresholds:\n",
        "    y_pred_temp = (combined_prob > thresh).astype(int)\n",
        "    current_f1 = f1_score(y_test, y_pred_temp)\n",
        "    if current_f1 > best_f1:\n",
        "        best_f1 = current_f1\n",
        "        best_thresh = thresh\n",
        "\n",
        "print(f\"\\nBest threshold: {best_thresh:.2f} with F1-Score: {best_f1:.4f}\")\n",
        "\n",
        "# Final predictions using the best threshold\n",
        "y_pred_final = (combined_prob > best_thresh).astype(int)\n",
        "\n",
        "\n",
        "# 6. Evaluation Metrics\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_final)\n",
        "precision = precision_score(y_test, y_pred_final)\n",
        "recall = recall_score(y_test, y_pred_final)\n",
        "f1 = f1_score(y_test, y_pred_final)\n",
        "roc_auc = roc_auc_score(y_test, combined_prob)\n",
        "\n",
        "print(\"\\nFinal Evaluation:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm2xSB61K_Je"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
